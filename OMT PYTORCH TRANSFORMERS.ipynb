{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n",
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_transformers import BertTokenizer,BertConfig, BertModel, BertForSequenceClassification, AdamW, WarmupLinearSchedule\n",
    "from tqdm import trange\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_file = 'omt_train_tab.csv'\n",
    "test_file = 'omt_test_tab.csv'\n",
    "output_dir = 'bert_for_seq_out'\n",
    "cache_dir = os.path.join(output_dir, 'cache')\n",
    "task_name = 'omt-classification'\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "    print('Directory', output_dir, ' created')\n",
    "except FileExistsError:\n",
    "        print('Directory already exists')\n",
    "try:\n",
    "    os.mkdir(cache_dir)\n",
    "    print('Directory', output_dir, ' created')\n",
    "except FileExistsError:\n",
    "        print('Directory already exists')\n",
    "        \n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    f1 = f1_score(y_true = labels, y_pred = preds, average='micro')\n",
    "    return{\n",
    "        'acc': acc,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_padd_trunc(tokenizer, corpus, max_len = 32, num_sentences = 1):\n",
    "    truncate_or_pad = max_len -2\n",
    "    doclist=[]\n",
    "    prefix = '[CLS]'\n",
    "    suffix = '[SEP]'\n",
    "    if num_sentences == 1:\n",
    "        i = 0\n",
    "        for doc in corpus:           \n",
    "            new_list = []\n",
    "            new_list.append(prefix)\n",
    "            doc = tokenizer.tokenize(doc)\n",
    "            if len(doc)> truncate_or_pad:\n",
    "                doc = doc[:truncate_or_pad]\n",
    "            for tok in doc:\n",
    "                new_list.append(tok)\n",
    "            new_list.append(suffix)\n",
    "            doclist.append(new_list)\n",
    "    if num_sentences == 2:\n",
    "        return doclist\n",
    "        #TO DO: Handle the two inputs\n",
    "    if num_sentences != 1 and num_sentences !=2:\n",
    "        print('Choose num_sentences between one and two please')\n",
    "    return doclist\n",
    "\n",
    "data = pd.read_csv(filepath_or_buffer=train_file, sep='\\t', header=None)\n",
    "\n",
    "MAX_LEN = 64 \n",
    "\n",
    "\n",
    "\n",
    "basic_tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased', do_lower_case=False)\n",
    "tokenized_texts = tokenize_padd_trunc(basic_tokenizer, data[0], max_len= MAX_LEN)\n",
    "\n",
    "labels = data[1].tolist()\n",
    "labels_vals = list(set(data[1].values))\n",
    "num_labels = len(labels_vals)\n",
    "label_map = {label: i for i, label in enumerate(labels_vals)}\n",
    "\n",
    "\n",
    "batchsize=90\n",
    "epochs = 5\n",
    "train_examples = len(tokenized_texts)\n",
    "num_train_optimization_steps = train_examples // batchsize\n",
    "\n",
    "#from run_glue.py\n",
    "\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "save_steps = 50\n",
    "logging_steps = 50\n",
    "max_steps = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_train, data_dev = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "input_ids = pad_sequences([basic_tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                         maxlen=MAX_LEN, value = 0, dtype= 'long', truncating='post')\n",
    "label_ids = [label_map[l] for l in labels]\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "token_type_ids = [[0 for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tr_inputs, val_inputs, tr_label, val_label = train_test_split(input_ids, label_ids, test_size = 0.1, random_state = 2019)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state = 2019, test_size = 0.1)\n",
    "tr_token_type_ids, val_token_type_ids, _, _ = train_test_split(token_type_ids, input_ids, random_state = 2019, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146579 9381056\n",
      "Max len worked fine, all tokentype id = 0\n"
     ]
    }
   ],
   "source": [
    "nichtnull = 0\n",
    "docs = 0\n",
    "for ii in token_type_ids:\n",
    "    if(len(ii) == MAX_LEN):\n",
    "        docs = docs + 1\n",
    "    for i in ii:\n",
    "        if i == 0:\n",
    "            nichtnull = nichtnull+1\n",
    "print(docs, nichtnull)\n",
    " \n",
    "if (nichtnull / MAX_LEN) == docs:\n",
    "    print('Max len worked fine, all tokentype id = 0')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs_tensor = torch.tensor(tr_inputs, dtype=torch.long)\n",
    "val_inputs_tensor = torch.tensor(val_inputs, dtype=torch.long)\n",
    "tr_label_tensor = torch.tensor(tr_label, dtype=torch.long)\n",
    "val_label_tensor = torch.tensor(val_label, dtype=torch.long)\n",
    "tr_masks_tensor = torch.tensor(tr_masks, dtype=torch.long)\n",
    "val_masks_tensor = torch.tensor(val_masks, dtype=torch.long)\n",
    "tr_token_type_tensor = torch.tensor(tr_token_type_ids, dtype=torch.long)\n",
    "val_token_type_tensor = torch.tensor(val_token_type_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_sampler, train_dataloader = [],[],[]\n",
    "train_data = TensorDataset(tr_inputs_tensor, tr_masks_tensor, tr_token_type_tensor, tr_label_tensor)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batchsize)\n",
    "\n",
    "valid_data, valid_sampler, valid_dataloader = [],[],[]\n",
    "valid_data = TensorDataset(val_inputs_tensor, val_masks_tensor, val_token_type_tensor, val_label_tensor)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler= valid_sampler, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = []\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-german-cased', cache_dir = cache_dir, num_labels = num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)]}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4684773057516242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  12%|█▎        | 1/8 [23:59<2:47:57, 1439.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.41748140834591874\n",
      "Validation accuracy: 0.8490244235229909\n",
      "F1- Score: 0.8490244235229909\n",
      "Train loss: 0.3656677764828989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 2/8 [48:02<2:24:03, 1440.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4216595947559626\n",
      "Validation accuracy: 0.8446582071223905\n",
      "F1- Score: 0.8446582071223905\n",
      "Train loss: 0.28262221003091026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  38%|███▊      | 3/8 [1:12:06<2:00:08, 1441.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.44905696297350106\n",
      "Validation accuracy: 0.8458179833538\n",
      "F1- Score: 0.8458179833538001\n",
      "Train loss: 0.2001722648201096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 4/8 [1:36:10<1:36:09, 1442.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5139184189171879\n",
      "Validation accuracy: 0.8428162095783872\n",
      "F1- Score: 0.8428162095783872\n",
      "Train loss: 0.14544192813133558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  62%|██████▎   | 5/8 [2:00:16<1:12:10, 1443.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5853581348079845\n",
      "Validation accuracy: 0.8260335652885796\n",
      "F1- Score: 0.8260335652885796\n",
      "Train loss: 0.11243087691820314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 6/8 [2:24:23<48:08, 1444.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6742031523055094\n",
      "Validation accuracy: 0.8347659980897804\n",
      "F1- Score: 0.8347659980897804\n",
      "Train loss: 0.09239296659244044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  88%|████████▊ | 7/8 [2:48:25<24:03, 1443.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6987789262291844\n",
      "Validation accuracy: 0.8292400054577705\n",
      "F1- Score: 0.8292400054577705\n",
      "Train loss: 0.07875737926533946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 8/8 [3:12:25<00:00, 1442.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7664080061064176\n",
      "Validation accuracy: 0.8271933415199891\n",
      "F1- Score: 0.827193341519989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 8\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    #train loop\n",
    "    model.zero_grad()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids':     batch[0],\n",
    "                 'attention_mask': batch[1],\n",
    "                 'token_type_ids': batch[2],\n",
    "                 'labels':         batch[3]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]    # since \"pytorch-transformer\" : all outputs are tuple\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm= 1)\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        #update params\n",
    "        \n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    #print loss per epoch\n",
    "    print('Train loss: {}'.format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    #eval\n",
    "    \n",
    "    results = {}\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps= 0\n",
    "    nb_eval_examples = 0\n",
    "    preds, out_label_ids = None,None\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps +=1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis = 0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis = 0)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    result = acc_and_f1(preds, out_label_ids)\n",
    "    results.update(result)\n",
    "    \n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation accuracy: {}\".format(results['acc']))\n",
    "    print(\"F1- Score: {}\".format(results['f1']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "#torch.save(model.bert.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.bert.load_state_dict(torch.load(os.path.join(output_dir, \"pytorch_model.bin\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(filepath_or_buffer=test_file, sep = '\\t', header = None)\n",
    "tokenized_texts_test = tokenize_padd_trunc(basic_tokenizer, data_test[0], max_len= MAX_LEN)\n",
    "labels_test = data_test[1].tolist()\n",
    "\n",
    "input_ids_test = pad_sequences([basic_tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
    "                         maxlen=MAX_LEN, value = 0, dtype= 'long', truncating='post')\n",
    "label_ids_test = [label_map[l] for l in labels_test]\n",
    "attention_masks_test = [[float(i>0) for i in ii] for ii in input_ids_test]\n",
    "token_type_ids_test = [[0 for i in ii] for ii in input_ids_test]\n",
    "\n",
    "test_inputs_tensor = torch.tensor(input_ids_test, dtype=torch.long)\n",
    "test_label_tensor = torch.tensor(label_ids_test, dtype=torch.long)\n",
    "test_masks_tensor = torch.tensor(attention_masks_test, dtype=torch.long)\n",
    "test_token_type_tensor = torch.tensor(token_type_ids_test, dtype=torch.long)\n",
    "\n",
    "test_data, test_sampler, test_dataloader = [],[],[]\n",
    "test_data = TensorDataset(test_inputs_tensor, test_masks_tensor, test_token_type_tensor, test_label_tensor)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7837988262628101\n",
      "Validation accuracy: 0.8231773320598536\n",
      "F1- Score: 0.8231773320598536\n"
     ]
    }
   ],
   "source": [
    "## test the model with unseen test data\n",
    "model.eval()\n",
    "test_results = {}\n",
    "test_loss = 0.0\n",
    "nb_test_steps= 0\n",
    "nb_test_examples = 0\n",
    "preds, out_label_ids = None,None\n",
    "   \n",
    "for batch in test_dataloader:\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2],\n",
    "                  'labels':         batch[3]}\n",
    "        outputs = model(**inputs)\n",
    "        tmp_test_loss, logits = outputs[:2]\n",
    "            \n",
    "        test_loss += tmp_test_loss.mean().item()\n",
    "    nb_test_steps +=1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis = 0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis = 0)\n",
    "    \n",
    "test_loss = test_loss / nb_test_steps\n",
    "preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "result = acc_and_f1(preds, out_label_ids)\n",
    "test_results.update(result)\n",
    "    \n",
    "print(\"Validation loss: {}\".format(test_loss))\n",
    "print(\"Validation accuracy: {}\".format(test_results['acc']))\n",
    "print(\"F1- Score: {}\".format(test_results['f1']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
