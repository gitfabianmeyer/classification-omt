{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_German_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Class</th>\n",
       "      <th>Subclass</th>\n",
       "      <th>answer_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181494</th>\n",
       "      <td>15471114907050152515K20</td>\n",
       "      <td>Sie versucht der anderen Person zu Helfen Löse...</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>23</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69605</th>\n",
       "      <td>3241342197844644BG</td>\n",
       "      <td>sie möchte immer höher hinaus, sie gibt alles ...</td>\n",
       "      <td>L</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>18</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129925</th>\n",
       "      <td>6581431170681727K</td>\n",
       "      <td>Sie will verhindern, dass Fehler erneut auftre...</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104570</th>\n",
       "      <td>10691401290219549K</td>\n",
       "      <td>Ich habe das Gefühl, diese Person bräuchte ein...</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>146</td>\n",
       "      <td>22</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68925</th>\n",
       "      <td>1361353255679140M9</td>\n",
       "      <td>Problem lösen, erklären was passiert ist. schl...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>15</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              #  \\\n",
       "181494  15471114907050152515K20   \n",
       "69605        3241342197844644BG   \n",
       "129925        6581431170681727K   \n",
       "104570       10691401290219549K   \n",
       "68925        1361353255679140M9   \n",
       "\n",
       "                                                  Answers Class Subclass  \\\n",
       "181494  Sie versucht der anderen Person zu Helfen Löse...     M        2   \n",
       "69605   sie möchte immer höher hinaus, sie gibt alles ...     L        2   \n",
       "129925  Sie will verhindern, dass Fehler erneut auftre...     M        4   \n",
       "104570  Ich habe das Gefühl, diese Person bräuchte ein...     A        5   \n",
       "68925   Problem lösen, erklären was passiert ist. schl...     M        5   \n",
       "\n",
       "        answer_len  word_count language  \n",
       "181494         146          23       de  \n",
       "69605          110          18       de  \n",
       "129925          99          15       de  \n",
       "104570         146          22       de  \n",
       "68925          101          15       de  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_German_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "from time import time\n",
    "import sys\n",
    "t0 = time()\n",
    "\n",
    "# to use my own preprocessing in the vectorizer later\n",
    "def my_dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#download stopwords, add 'dass' for new spellins \n",
    "stopWords = set(stopwords.words('german'))\n",
    "stopWords.add('dass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizes a list of strings\n",
    "#return: list of tokens\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text, language = 'german')\n",
    "    return tokens\n",
    "\n",
    "#retrns: list of strings which do not occure in stopWords\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if not w in stopWords]\n",
    "\n",
    "\n",
    "pattern = re.compile(\"\\-|\\!|\\:|\\.|\\d|\\,|\\:|\\(|\\)|\\?|\\\"|\\\\|\\+|\\%|\\/\")\n",
    "def remove_patterns_from_string(strg):\n",
    "    return re.sub(pattern, \" \", strg)\n",
    "\n",
    "# in : row for row from vector\n",
    "# out: preprocessed version of the row\n",
    "def preprocess_text(corpus):\n",
    "        helplist = []\n",
    "        for doc in corpus:\n",
    "            #only user lower case\n",
    "            doc = doc.lower()\n",
    "            doc = remove_patterns_from_string(doc)\n",
    "            doc = tokenize_text(doc)\n",
    "            doc = remove_stopwords(doc)\n",
    "            helplist.append(doc)\n",
    "        return helplist\n",
    "\n",
    "#split it into X ( = input) and y ( the labels)\n",
    "X_train, y_train = preprocess_text(df_German_train[\"Answers\"]), df_German_train[\"Class\"]\n",
    "\n",
    "import random\n",
    "print(random.sample(X_train, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['testcorpus']]\n"
     ]
    }
   ],
   "source": [
    "teste = ['ich bin ein testcorpus']\n",
    "print(preprocess_text(teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_German_test\n",
    "X_test, y_test = preprocess_text(df_German_test[\"Answers\"]), df_German_test[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                      Type                          Data/Info\n",
      "---------------------------------------------------------------------\n",
      "CountVectorizer               type                          <class 'sklearn.feature_e<...>on.text.CountVectorizer'>\n",
      "FeatureUnion                  ABCMeta                       <class 'sklearn.pipeline.FeatureUnion'>\n",
      "GridSearchCV                  ABCMeta                       <class 'sklearn.model_sel<...>on._search.GridSearchCV'>\n",
      "LinearSVC                     type                          <class 'sklearn.svm.classes.LinearSVC'>\n",
      "MultinomialNB                 ABCMeta                       <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "Pipeline                      ABCMeta                       <class 'sklearn.pipeline.Pipeline'>\n",
      "TfidfVectorizer               type                          <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "X_test                        list                          n=62820\n",
      "X_train                       list                          n=146579\n",
      "best_parameters               dict                          n=24\n",
      "biterms                       list                          n=73289\n",
      "classification_report         function                      <function classification_<...>rt at 0x000002138C63C2F0>\n",
      "codecs                        module                        <module 'codecs' from 'C:<...>ion_omt\\\\lib\\\\codecs.py'>\n",
      "df_German_test                DataFrame                                              <...>n[62820 rows x 7 columns]\n",
      "df_German_train               DataFrame                                              <...>[146579 rows x 7 columns]\n",
      "grid_search1                  GridSearchCV                  GridSearchCV(cv=5, error_<...>ng='accuracy', verbose=1)\n",
      "i                             int                           62819\n",
      "m_labels                      list                          n=62820\n",
      "my_dummy                      function                      <function my_dummy at 0x000002139650BE18>\n",
      "nltk                          module                        <module 'nltk' from 'C:\\\\<...>ages\\\\nltk\\\\__init__.py'>\n",
      "np                            module                        <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                            module                        <module 'os' from 'C:\\\\Us<...>ication_omt\\\\lib\\\\os.py'>\n",
      "param_name                    str                           vect1__ngram_range\n",
      "parameters1                   dict                          n=5\n",
      "pattern                       SRE_Pattern                   re.compile('\\\\-|\\\\!|\\\\:|\\<...>\\)|\\\\?|\"|\\\\|\\\\+|\\\\%|\\\\/')\n",
      "pd                            module                        <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pipeline1                     Pipeline                      Pipeline(memory=None,\\n  <...>=None, fit_prior=True))])\n",
      "powermotive                   Series                        0        M\\n1        M\\n2<...>gth: 62820, dtype: object\n",
      "predictions                   ndarray                       62820: 62820 elems, type `<U1`, 251280 bytes (245.390625 kb)\n",
      "preprocess_text               function                      <function preprocess_text at 0x00000213A2A4D158>\n",
      "re                            module                        <module 're' from 'C:\\\\Us<...>ication_omt\\\\lib\\\\re.py'>\n",
      "remove_patterns_from_string   function                      <function remove_patterns<...>ng at 0x00000213A2A4D0D0>\n",
      "remove_stopwords              function                      <function remove_stopwords at 0x00000213A2A4D048>\n",
      "stopWords                     set                           {'welches', 'einem', 'sol<...>aus', 'sondern', 'ander'}\n",
      "stopwords                     WordListCorpusReader          <WordListCorpusReader in <...>ata\\\\corpora\\\\stopwords'>\n",
      "sys                           module                        <module 'sys' (built-in)>\n",
      "t0                            float                         1559894828.5397213\n",
      "time                          builtin_function_or_method    <built-in function time>\n",
      "time_svc                      float                         1559919440.9642344\n",
      "tokenize_text                 function                      <function tokenize_text at 0x0000021395E23F28>\n",
      "train_test_split              function                      <function train_test_split at 0x000002138C620730>\n",
      "vec                           CountVectorizer               CountVectorizer(analyzer=<...>n        vocabulary=None)\n",
      "vec_to_biterms                function                      <function vec_to_biterms at 0x000002139650BEA0>\n",
      "word_tokenize                 function                      <function word_tokenize at 0x000002138C8C1950>\n",
      "xgb                           module                        <module 'xgboost' from 'C<...>s\\\\xgboost\\\\__init__.py'>\n",
      "y_biterms1                    Series                        98874     M\\n54423     M\\<...>gth: 73289, dtype: object\n",
      "y_biterms2                    Series                        17594     M\\n207593    0\\<...>gth: 73290, dtype: object\n",
      "y_test                        Series                        63231     M\\n104740    A\\<...>gth: 62820, dtype: object\n",
      "y_train                       Series                        181494    M\\n69605     L\\<...>th: 146579, dtype: object\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the ML Part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2970\n",
      "           A       0.00      0.00      0.00     10607\n",
      "           L       0.00      0.00      0.00     12270\n",
      "           M       0.59      1.00      0.74     36973\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     62820\n",
      "   macro avg       0.15      0.25      0.19     62820\n",
      "weighted avg       0.35      0.59      0.44     62820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# minimal classification performance: all answers set to M ( ZeroN - Classifier)\n",
    "#\n",
    "m_labels = []\n",
    "for i in range(len(y_test)):\n",
    "    m_labels.append(\"M\")\n",
    "powermotive = pd.Series(m_labels)\n",
    "print(classification_report(y_test, powermotive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 56.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3399.514s\n",
      "\n",
      "Best score: 0.827\n",
      "best parameters set:\n",
      "\tclf1__C: 0.1\n",
      "\tclf1__max_iter: 300\n",
      "\tvect1__max_df: 1.0\n",
      "\tvect1__max_features: None\n",
      "\tvect1__min_df: 1\n",
      "\tvect1__ngram_range: (1, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.22      0.31      2970\n",
      "           A       0.80      0.77      0.78     10607\n",
      "           L       0.81      0.77      0.79     12270\n",
      "           M       0.85      0.91      0.88     36973\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     62820\n",
      "   macro avg       0.75      0.67      0.69     62820\n",
      "weighted avg       0.82      0.83      0.82     62820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell trains SVC classifier with different features (tf, tfidf, min/max _docfrequency, kernels, C, iterations )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = []\n",
    "grid_search1 = []\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect1', CountVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),                     \n",
    "    ('clf1', LinearSVC()),    \n",
    "])\n",
    "\n",
    "parameters1 = {\n",
    "    'vect1__max_df': (1.0,0.9,0.8, 0.5),\n",
    "    'vect1__ngram_range':((1,1),(1,2)),\n",
    "    'vect1__min_df': (1,5,10),\n",
    "    'vect1__max_features': (10000,20000,30000, None), \n",
    "    'clf1__C':(0.1,),\n",
    "    'clf1__max_iter':(300,),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search1 = GridSearchCV(pipeline1, parameters1, cv=5,\n",
    "                           n_jobs = -1, verbose = 1, scoring='accuracy')\n",
    "    print(\"performing gridsearch...\")\n",
    "    time_svc = time()\n",
    "    grid_search1.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search1.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    predictions = grid_search1.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "done in 171.799s\n",
      "\n",
      "Best score: 0.827\n",
      "best parameters set:\n",
      "\tclf1__C: 0.1\n",
      "\tclf1__max_iter: 300\n",
      "\tvect1__max_df: 0.9\n",
      "\tvect1__max_features: None\n",
      "\tvect1__min_df: 5\n",
      "\tvect1__ngram_range: (1, 1)\n",
      "\tvect1__use_idf: True\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.15      0.24      2970\n",
      "           A       0.82      0.75      0.78     10607\n",
      "           L       0.82      0.75      0.79     12270\n",
      "           M       0.83      0.93      0.88     36973\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     62820\n",
      "   macro avg       0.78      0.64      0.67     62820\n",
      "weighted avg       0.82      0.83      0.81     62820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell trains SVC classifier with different features (tf, tfidf, min/max _docfrequency, kernels, C, iterations )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = []\n",
    "grid_search1 = []\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect1', TfidfVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),\n",
    "    ('clf1', LinearSVC()),    \n",
    "])\n",
    "\n",
    "parameters1 = {\n",
    "    'vect1__max_df': (0.9,),\n",
    "    'vect1__ngram_range':((1,1),),\n",
    "    'vect1__min_df': (5,10,15),\n",
    "    'vect1__max_features': (None,), \n",
    "    'vect1__use_idf': (False, True),\n",
    "    'clf1__C':(0.1,),\n",
    "    'clf1__max_iter':(300,),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search1 = GridSearchCV(pipeline1, parameters1, cv=5,\n",
    "                           n_jobs = -1, scoring='accuracy')\n",
    "    print(\"performing gridsearch...\")\n",
    "    time_svc = time()\n",
    "    grid_search1.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search1.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    predictions = grid_search1.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 192.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 12317.911s\n",
      "\n",
      "Best score: 0.818\n",
      "best parameters set:\n",
      "\tclf1__colsample_bytree: 0.7\n",
      "\tclf1__learning_rate: 0.1\n",
      "\tclf1__max_depth: 6\n",
      "\tclf1__min_child_weight: 11\n",
      "\tclf1__n_estimators: 1000\n",
      "\tclf1__num_class: 4\n",
      "\tclf1__objective: 'multi:softprob'\n",
      "\tclf1__silent: 1\n",
      "\tclf1__subsample: 0.8\n",
      "\tvect1__max_df: 1.0\n",
      "\tvect1__max_features: None\n",
      "\tvect1__min_df: 5\n",
      "\tvect1__ngram_range: (1, 1)\n",
      "\tvect1__use_idf: False\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.19      0.29      2970\n",
      "           A       0.83      0.72      0.77     10607\n",
      "           L       0.82      0.73      0.77     12270\n",
      "           M       0.82      0.93      0.87     36973\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     62820\n",
      "   macro avg       0.76      0.64      0.68     62820\n",
      "weighted avg       0.81      0.82      0.81     62820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell trains SVC classifier with different features (tf, tfidf, min/max _docfrequency, kernels, C, iterations )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline1 = []\n",
    "grid_search1 = []\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect1', TfidfVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),\n",
    "    ('clf1', xgb.XGBClassifier()),    \n",
    "])\n",
    "\n",
    "parameters1 = {\n",
    "    'vect1__max_df': (1.0, 0.9),\n",
    "    'vect1__ngram_range':((1,1),),\n",
    "    'vect1__min_df': (5,),\n",
    "    'vect1__max_features': (None, 20000,), \n",
    "    'vect1__use_idf': (True, False),\n",
    "    'clf1__objective':['multi:softmax',],\n",
    "    'clf1__num_class':[4],\n",
    "    'clf1__learning_rate': [0.1], #so called `eta` value\n",
    "    'clf1__max_depth': [6],\n",
    "    'clf1__min_child_weight': [11],\n",
    "    'clf1__silent': [1],\n",
    "    'clf1__subsample': [0.8],\n",
    "    'clf1__colsample_bytree': [0.7],\n",
    "    'clf1__n_estimators': [1000],\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search1 = GridSearchCV(pipeline1, parameters1, cv=5,\n",
    "                           n_jobs = -1, verbose = 1, scoring='accuracy')\n",
    "    print(\"performing gridsearch...\")\n",
    "    time_svc = time()\n",
    "    grid_search1.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search1.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    predictions = grid_search1.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 46.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 83.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 132.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 188.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 252.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15155.893s\n",
      "\n",
      "Best score: 0.820\n",
      "best parameters set:\n",
      "\tclf1__alpha: 0.5\n",
      "\tvect1__max_df: 1.0\n",
      "\tvect1__max_features: None\n",
      "\tvect1__min_df: 1\n",
      "\tvect1__ngram_range: (1, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.09      0.17      2970\n",
      "           A       0.78      0.77      0.78     10607\n",
      "           L       0.79      0.77      0.78     12270\n",
      "           M       0.84      0.91      0.87     36973\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     62820\n",
      "   macro avg       0.79      0.64      0.65     62820\n",
      "weighted avg       0.82      0.82      0.81     62820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell trains SVC classifier with different features (tf, tfidf, min/max _docfrequency, kernels, C, iterations )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = []\n",
    "grid_search1 = []\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect1', CountVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),                     \n",
    "    ('clf1', MultinomialNB()),    \n",
    "])\n",
    "\n",
    "parameters1 = {\n",
    "    'vect1__max_df': (1.0,0.9,0.8, 0.5),\n",
    "    'vect1__ngram_range':((1,1),(1,2)),\n",
    "    'vect1__min_df': (1,5,10),\n",
    "    'vect1__max_features': (10000,20000,30000, None), \n",
    "    'clf1__alpha': [0.001, 0.01, 0.1, 0.5, 1,]\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search1 = GridSearchCV(pipeline1, parameters1, cv=5,\n",
    "                           n_jobs = -1, verbose = 1, scoring='accuracy')\n",
    "    print(\"performing gridsearch...\")\n",
    "    time_svc = time()\n",
    "    grid_search1.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search1.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    predictions = grid_search1.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short helper for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73289"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_biterms1, X_biterms2, y_biterms1, y_biterms2 = train_test_split(X_train, y_train, test_size = 0.5)\n",
    "len(X_biterms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms\n",
    "def my_dummy(doc):\n",
    "    return doc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # vectorize texts\n",
    "    vec = CountVectorizer(min_df = 4, max_df = 0.8, preprocessor = my_dummy,\n",
    "                                                              tokenizer =my_dummy, \n",
    "                                                              token_pattern=None)\n",
    "    Xbtm = vec.fit_transform(X_biterms1).toarray()\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(Xbtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-35bfb0c9824e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXbtm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_biterms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_biterms2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "del vocab, Xbtm, X_biterms1, X_biterms2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                      Type                          Data/Info\n",
      "---------------------------------------------------------------------\n",
      "CountVectorizer               type                          <class 'sklearn.feature_e<...>on.text.CountVectorizer'>\n",
      "FeatureUnion                  ABCMeta                       <class 'sklearn.pipeline.FeatureUnion'>\n",
      "GridSearchCV                  ABCMeta                       <class 'sklearn.model_sel<...>on._search.GridSearchCV'>\n",
      "LinearSVC                     type                          <class 'sklearn.svm.classes.LinearSVC'>\n",
      "MultinomialNB                 ABCMeta                       <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "Pipeline                      ABCMeta                       <class 'sklearn.pipeline.Pipeline'>\n",
      "TfidfVectorizer               type                          <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "X_test                        list                          n=62820\n",
      "X_train                       list                          n=146579\n",
      "best_parameters               dict                          n=24\n",
      "biterms                       list                          n=73289\n",
      "classification_report         function                      <function classification_<...>rt at 0x000002138C63C2F0>\n",
      "codecs                        module                        <module 'codecs' from 'C:<...>ion_omt\\\\lib\\\\codecs.py'>\n",
      "df_German_test                DataFrame                                              <...>n[62820 rows x 7 columns]\n",
      "df_German_train               DataFrame                                              <...>[146579 rows x 7 columns]\n",
      "grid_search1                  GridSearchCV                  GridSearchCV(cv=5, error_<...>ng='accuracy', verbose=1)\n",
      "i                             int                           62819\n",
      "m_labels                      list                          n=62820\n",
      "my_dummy                      function                      <function my_dummy at 0x000002139650BE18>\n",
      "nltk                          module                        <module 'nltk' from 'C:\\\\<...>ages\\\\nltk\\\\__init__.py'>\n",
      "np                            module                        <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                            module                        <module 'os' from 'C:\\\\Us<...>ication_omt\\\\lib\\\\os.py'>\n",
      "param_name                    str                           vect1__ngram_range\n",
      "parameters1                   dict                          n=5\n",
      "pattern                       SRE_Pattern                   re.compile('\\\\-|\\\\!|\\\\:|\\<...>\\)|\\\\?|\"|\\\\|\\\\+|\\\\%|\\\\/')\n",
      "pd                            module                        <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pipeline1                     Pipeline                      Pipeline(memory=None,\\n  <...>=None, fit_prior=True))])\n",
      "powermotive                   Series                        0        M\\n1        M\\n2<...>gth: 62820, dtype: object\n",
      "predictions                   ndarray                       62820: 62820 elems, type `<U1`, 251280 bytes (245.390625 kb)\n",
      "preprocess_text               function                      <function preprocess_text at 0x00000213A2A4D158>\n",
      "re                            module                        <module 're' from 'C:\\\\Us<...>ication_omt\\\\lib\\\\re.py'>\n",
      "remove_patterns_from_string   function                      <function remove_patterns<...>ng at 0x00000213A2A4D0D0>\n",
      "remove_stopwords              function                      <function remove_stopwords at 0x00000213A2A4D048>\n",
      "stopWords                     set                           {'welches', 'einem', 'sol<...>aus', 'sondern', 'ander'}\n",
      "stopwords                     WordListCorpusReader          <WordListCorpusReader in <...>ata\\\\corpora\\\\stopwords'>\n",
      "sys                           module                        <module 'sys' (built-in)>\n",
      "t0                            float                         1559894828.5397213\n",
      "time                          builtin_function_or_method    <built-in function time>\n",
      "time_svc                      float                         1559919440.9642344\n",
      "tokenize_text                 function                      <function tokenize_text at 0x0000021395E23F28>\n",
      "train_test_split              function                      <function train_test_split at 0x000002138C620730>\n",
      "vec                           CountVectorizer               CountVectorizer(analyzer=<...>n        vocabulary=None)\n",
      "vec_to_biterms                function                      <function vec_to_biterms at 0x000002139650BEA0>\n",
      "word_tokenize                 function                      <function word_tokenize at 0x000002138C8C1950>\n",
      "xgb                           module                        <module 'xgboost' from 'C<...>s\\\\xgboost\\\\__init__.py'>\n",
      "y_biterms1                    Series                        98874     M\\n54423     M\\<...>gth: 73289, dtype: object\n",
      "y_biterms2                    Series                        17594     M\\n207593    0\\<...>gth: 73290, dtype: object\n",
      "y_test                        Series                        63231     M\\n104740    A\\<...>gth: 62820, dtype: object\n",
      "y_train                       Series                        181494    M\\n69605     L\\<...>th: 146579, dtype: object\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
