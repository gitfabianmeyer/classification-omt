{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, codecs\n",
    "from nltk import word_tokenize\n",
    "from time import time\n",
    "import sys\n",
    "t0 = time()\n",
    "def my_dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      "#           1000 non-null object\n",
      "Answers     1000 non-null object\n",
      "Class       1000 non-null object\n",
      "Subclass    1000 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.3+ KB\n",
      "None \n",
      "\n",
      "M     518\n",
      "L     194\n",
      "A     192\n",
      "0      91\n",
      "\\N      5\n",
      "Name: Class, dtype: int64 \n",
      "\n",
      "after cleaning...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995 entries, 0 to 994\n",
      "Data columns (total 5 columns):\n",
      "index       995 non-null int64\n",
      "#           995 non-null object\n",
      "Answers     995 non-null object\n",
      "Class       995 non-null object\n",
      "Subclass    995 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 38.9+ KB\n",
      "None\n",
      "M    518\n",
      "L    194\n",
      "A    192\n",
      "0     91\n",
      "Name: Class, dtype: int64 \n",
      "\n",
      "<bound method NDFrame.head of      index                               #  \\\n",
      "0        0        185124138106620081063215   \n",
      "1        1        185124138106620081063215   \n",
      "2        2        185124138106620081063215   \n",
      "3        3      52122529750377312346781011   \n",
      "4        4      52122529750377312346781011   \n",
      "5        5      52122529750377312346781011   \n",
      "6        6      52122529750377312346781011   \n",
      "7        7      52122529750377312346781011   \n",
      "8        8      52122529750377312346781011   \n",
      "9        9      52122529750377312346781011   \n",
      "10      10      52122529750377312346781011   \n",
      "11      11      52122529750377312346781011   \n",
      "12      12      52122529750377312346781011   \n",
      "13      13      52122529750377312346781011   \n",
      "14      14      52122529750377312346781011   \n",
      "15      15      52122529750377312346781011   \n",
      "16      16      52122529750377312346781011   \n",
      "17      17      52122529750377312346781011   \n",
      "18      18    4812248500709191234567891011   \n",
      "19      19    4812248500709191234567891011   \n",
      "20      20    4812248500709191234567891011   \n",
      "21      21    4812248500709191234567891011   \n",
      "22      22    4812248500709191234567891011   \n",
      "23      23    4812248500709191234567891011   \n",
      "24      24    4812248500709191234567891011   \n",
      "25      25    4812248500709191234567891011   \n",
      "26      26    4812248500709191234567891011   \n",
      "27      27    4812248500709191234567891011   \n",
      "28      28    4812248500709191234567891011   \n",
      "29      29    4812248500709191234567891011   \n",
      "..     ...                             ...   \n",
      "965    970    9012333411730141234567891011   \n",
      "966    971    9012333411730141234567891011   \n",
      "967    972    9012333411730141234567891011   \n",
      "968    973    9012333411730141234567891011   \n",
      "969    974    9012333411730141234567891011   \n",
      "970    975    9012333411730141234567891011   \n",
      "971    976    1001231614795044123456781011   \n",
      "972    977    1001231614795044123456781011   \n",
      "973    978    1001231614795044123456781011   \n",
      "974    979    1001231614795044123456781011   \n",
      "975    980    1001231614795044123456781011   \n",
      "976    981    1001231614795044123456781011   \n",
      "977    982    1001231614795044123456781011   \n",
      "978    983    1001231614795044123456781011   \n",
      "979    984    1001231614795044123456781011   \n",
      "980    985    1001231614795044123456781011   \n",
      "981    986    1001231614795044123456781011   \n",
      "982    987    1001231614795044123456781011   \n",
      "983    988    1001231614795044123456781011   \n",
      "984    989    1001231614795044123456781011   \n",
      "985    990    1001231614795044123456781011   \n",
      "986    991  951234887222659123456781011      \n",
      "987    992  951234887222659123456781011      \n",
      "988    993  951234887222659123456781011      \n",
      "989    994  951234887222659123456781011      \n",
      "990    995  951234887222659123456781011      \n",
      "991    996  951234887222659123456781011      \n",
      "992    997  951234887222659123456781011      \n",
      "993    998  951234887222659123456781011      \n",
      "994    999  951234887222659123456781011      \n",
      "\n",
      "                                               Answers Class Subclass  \n",
      "0    ignoranz den anderen gegenüber.schlecht.die Pe...     M        5  \n",
      "1    mitlachen, mit eingeschlossen zu werden.sie la...     A        5  \n",
      "2    den anderen Umamen, ehrlichkeit, vertrauen.gut...     A        1  \n",
      "3    Sie hält die andere Person, stütz sie. Gut.Sie...     M        1  \n",
      "4    Entspannung, Spaß zu haben. Sie albern herum. ...     A        2  \n",
      "5    Sie unterhalten sich über Dinge, die die die d...     0        0  \n",
      "6    Sie redet aktiv auf die zweite Person ein. übe...     M        4  \n",
      "7    Ruhe zu haben, sie denkt nach.nachdenklich.Sie...     0        0  \n",
      "8    Halt zu kriegen, nach oben zu kommen, Sie klet...     L        3  \n",
      "9    Der zweiten Person zu helfen die \"Kopfnuss\" zu...     M        1  \n",
      "10   Sie beurteilt ein Arbeitsergebnis.mächtig, kon...     M        3  \n",
      "11   Sie bauen ein Gerät zusammen. Wichtig ist die ...     0        0  \n",
      "12   Der zweiten Person zu erklären das Sie einen F...     M        4  \n",
      "13   Der zweite Person Anerkennung zu zeigen..stolz...     M        2  \n",
      "14   Sie hebnt ein großes Gewicht und fühlt sich al...     M        2  \n",
      "15   Der anderen Person seine Ergbenisse zu erläute...     L        2  \n",
      "16   Sie gibt der Gruppe Anweisungen. Wichtig ist I...     M        3  \n",
      "17   Sie mäßregelt die kleinere Person. Wichtig ist...     M        4  \n",
      "18   sie beschützt.gut.weil sie etwas für den ander...     A        3  \n",
      "19   sie reden miteinander.fröhlich.weil sie mitein...     A        2  \n",
      "20   sie lästern über den dritten.unsicher.weil sie...     A        5  \n",
      "21   sie versucht sich zu verteidigen.wie eingesper...     M        3  \n",
      "22   sie denkt nach. für sie ist es wichtig nachzud...     L        1  \n",
      "23   sie will sich retten.in gefahr.weil sie fast r...     L        5  \n",
      "24   sie will ein rätsel lösen.beschützt.weil der a...     L        3  \n",
      "25   für sie ist wichtig dass das kind mal ein arzt...     A        3  \n",
      "26   sie warten dass sie nach hause dürfen.gelangwe...     L        4  \n",
      "27   sie versucht sich zu verteidigen.wütend.weil s...     L        4  \n",
      "28   sie versucht einen auf lieb zu tun.dem anderen...     M        2  \n",
      "29   die einen klatschen, die anderen sind besorgt,...     L        1  \n",
      "..                                                 ...   ...      ...  \n",
      "965  klartext zu reden. anschreien.sicher in dem wa...     M        4  \n",
      "966  das man ihr zuhöhr. auf eine person einreden.g...     M        2  \n",
      "967  was abzugeben. etwas hochhalten.stark.weil sie...     L        2  \n",
      "968  ruhig zu bleiben. rihig da sitzen.hilflos.weil...     M        5  \n",
      "969  das sie das problem vieleicht lösen. aleine da...     A        5  \n",
      "970  auch angehöhrt zu werden. sie schämt sich.alei...     M        5  \n",
      "971  sie möchte gehalten werden und liegt im arm de...     A        4  \n",
      "972  die person lacht sehr laut, sie möchte auf sic...     A        4  \n",
      "973  die person belauscht ein gepräch  sie weiß das...     0        0  \n",
      "974  die person unterhält sich und bietet ihre hand...     A        5  \n",
      "975  die person denkt nach  möchte ein problem löse...     0        0  \n",
      "976  die person erklimmt einen steilen berg  sie mö...     L        2  \n",
      "977  sie versucht eine aufgabe zu lösen, die ihr wi...     L        5  \n",
      "978  sie möchte, dass die andere person ihr verzeih...     M        5  \n",
      "979  sie wollen eine maschine bauen und schneller s...     L        4  \n",
      "980  die person schimpft, weil die andere person ei...     M        4  \n",
      "981  die person möchte auch einmal so groß, stark u...     M        5  \n",
      "982  die person hat einen preis gewonnen und möchte...     M        2  \n",
      "983  die person möchte einerseits mit der anderen p...     M        4  \n",
      "984  sie möchte ein thema vortragen, für das sie si...     M        4  \n",
      "985  die peron schaut zu boden, indem sie am liebst...     M        5  \n",
      "986  Es ist eine Mutter, die ihr Kind umarmt.aktiv,...     M        3  \n",
      "987  sie unterhalten sich über eine witzige Situati...     A        2  \n",
      "988  sie ist eine Außenseiterin und wird in der Kla...     M        5  \n",
      "989  sie wohnt in einer WG, die andere Person ist i...     M        4  \n",
      "990  sie steht an einer Bushaltestelle und friert.i...     A        5  \n",
      "991  sie klettert an einem Hügel hoch.angestrengt, ...     L        3  \n",
      "992  sie sieht sich ihre Bilder an, die andere Pers...     0        0  \n",
      "993  das ist eine Schülerin, die ihr Zeugnis ihrem ...     M        5  \n",
      "994  sie arbeiten auf einem Fließband.müde, konzent...     0        0  \n",
      "\n",
      "[995 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "#Load Data to pandas - frame vor better visualization\n",
    "import pandas as pd\n",
    "complete_data = pd.read_csv('omt_shortned.txt', sep = \";\",\n",
    "                           names = [\"#\",\"Answers\", \"Class\" , \"Subclass\"] )\n",
    "print('before cleaning...\\n')\n",
    "print(complete_data.info(), \"\\n\")\n",
    "print(complete_data[\"Class\"].value_counts(), \"\\n\")\n",
    "#delete eventually empty entries\n",
    "complete_data = complete_data.dropna(axis = 0)\n",
    "#delete eventually wrong labels ('\\N' seems to accure from time to time)\n",
    "complete_data = complete_data.drop(complete_data[complete_data.Class == '\\\\N'].index).reset_index()\n",
    "print('after cleaning...\\n')\n",
    "print(complete_data.info())\n",
    "print(complete_data[\"Class\"].value_counts(), \"\\n\")\n",
    "print(complete_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the needed infos from data_complete\n",
    "data = complete_data.filter([\"Answers\", \"Class\"],axis = 1)\n",
    "\n",
    "#split it into X ( = input) and y ( the labels)\n",
    "X, y = data[\"Answers\"], data[\"Class\"]\n",
    "\n",
    "#split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "#Bei größerem Trainingsset: stratified_shufflesplit nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['person', 'kurz', 'feierabend', 'unterhält', 'tag', '.', 'weiterzuarbeiten', 'gespräch', 'gleichwichtig', '.', 'unterhalten', 'erschöpft', '.', 'geschichte', 'person', 'anhört', 'gearbeitet', 'hat/noch', '.', 'gespräch', 'kollegen', 'bald', 'ende', 'person', 'macht', 'restliche', 'arbeit', 'fertig', 'geht', 'hause', '.'], ['macht', 'grösser', 'wichtig', '.', 'dominant', 'recht', '.', 'körperhaltung', ':', 'steht', ',', 'angewinkelte', 'arme', ',', 'kopf', 'leicht', 'zweiten', 'person', 'geneigt', '.', 'setzt', 'löst', 'augenhöhe', '.']]\n",
      "<class 'list'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# The pattern I don't want in my corpus \n",
    "pattern = re.compile(\"\\-|\\!|\\:|\\.|\\d|\\,|\\:|\\(|\\)|\\?|\\\"|\\\\|\\+|\\%|\\/\")\n",
    "\n",
    "#return: a patternless copy of the input string\n",
    "def insert_space_between_special_chars(strg):\n",
    "    return re.sub('([a-zA-Z])([^A-Za-z0-9]+)([a-zA-Z])', r'\\1\\2 \\3', strg)\n",
    "\n",
    "def remove_patterns_from_string(strg):\n",
    "    return re.sub(pattern, \" \", strg)\n",
    "\n",
    "# german stopwords (words who accure very often in texts and don't contribute anything to the text like articles, conjunctions)\n",
    "stopWords = set(stopwords.words(\"german\"))\n",
    "stopWords.add('dass')\n",
    "\n",
    "#tokenizes a list of strings\n",
    "#return: list of tokens\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text, language = 'german')\n",
    "    return tokens\n",
    "\n",
    "#retrns: list of strings which do not occure in stopWords\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if not w in stopWords]\n",
    "\n",
    "# in : row for row from vector\n",
    "# out: preprocessed version of the row\n",
    "def preprocess_text(corpus):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        #to change xyz.abc to xyz. abc\n",
    "        text = re.sub('([a-zA-Z])(\\.)([a-zA-Z])', r'\\1\\2 \\3', text)\n",
    "        text = text.lower()\n",
    "        #text = remove_patterns_from_string(text)\n",
    "        text = tokenize_text(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "    return normalized_corpus\n",
    "\n",
    "#later preprocess only X_train and X_val. X_test gets own pipeline in the end\n",
    "X_train_preprocessed = preprocess_text(X_train)\n",
    "X_test_preprocessed = preprocess_text(X_test)\n",
    "\n",
    "\n",
    "print(X_train_preprocessed[:2])\n",
    "print(type(X_train_preprocessed))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           A       0.00      0.00      0.00        65\n",
      "           L       0.00      0.00      0.00        71\n",
      "           M       0.56      1.00      0.72       222\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       398\n",
      "   macro avg       0.14      0.25      0.18       398\n",
      "weighted avg       0.31      0.56      0.40       398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "import pandas as pd\n",
    "\n",
    "# minimal classification performance: all answers M\n",
    "m_labels = []\n",
    "for i in range(len(y_test)):\n",
    "    m_labels.append(\"M\")\n",
    "machtmotiv = pd.Series(m_labels)\n",
    "#plot_classification_report(classification_report(y_test, machtmotiv))\n",
    "print(classification_report(y_test, machtmotiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n",
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "pipeline: ['vect1', 'tf1', 'clf1']\n",
      "parameters:\n",
      "{'vect1__max_df': (0.9, 1.0), 'vect1__ngram_range': ((1, 1), (1, 2), (1, 3)), 'vect1__min_df': (1, 2), 'tf1__use_idf': (True, False), 'clf1__kernel': ('linear', 'rbf'), 'clf1__C': (0.001, 0.1, 1, 10, 100), 'clf1__max_iter': (50, 100, 200)}\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 199.881s\n",
      "\n",
      "Best score: 0.714\n",
      "best parameters set:\n",
      "\tclf1__C: 10\n",
      "\tclf1__kernel: 'linear'\n",
      "\tclf1__max_iter: 100\n",
      "\ttf1__use_idf: False\n",
      "\tvect1__max_df: 0.9\n",
      "\tvect1__min_df: 1\n",
      "\tvect1__ngram_range: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed:  3.3min finished\n",
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn. decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect1', CountVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),\n",
    "                            \n",
    "    ('tf1',TfidfTransformer()),\n",
    "                            \n",
    "    ('clf1', SVC()),    \n",
    "])\n",
    "\n",
    "parameters1 = {\n",
    "    'vect1__max_df': (0.9, 1.0),\n",
    "    'vect1__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'vect1__min_df': (1,2),\n",
    "    'tf1__use_idf': (True, False),\n",
    "    'clf1__kernel': ('linear','rbf'), \n",
    "    'clf1__C':(0.001,0.1,1,10,100),\n",
    "    'clf1__max_iter':(50,100,200),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search1 = GridSearchCV(pipeline1, parameters1, cv=5,\n",
    "                           n_jobs = -1, verbose = 1)\n",
    "    print(\"performing gridsearch...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline1.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters1)\n",
    "    time_svc = time()\n",
    "    grid_search1.fit(X_train_preprocessed, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search1.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with SVC...\n",
      "performing gridsearch...\n",
      "pipeline: ['vect2', 'tf2', 'clf2']\n",
      "parameters:\n",
      "{'vect2__max_df': (0.9, 0.95, 1.0), 'vect2__ngram_range': ((1, 1), (1, 2), (1, 3)), 'vect2__min_df': (1, 2, 5), 'tf2__use_idf': (True, False), 'clf2__n_neighbors': (5, 10), 'clf2__algorithm': ('ball_tree', 'kd_tree'), 'clf2__leaf_size': (30, 50, 80)}\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 138.179s\n",
      "\n",
      "Best score: 0.688\n",
      "best parameters set:\n",
      "\tclf2__algorithm: 'ball_tree'\n",
      "\tclf2__leaf_size: 30\n",
      "\tclf2__n_neighbors: 10\n",
      "\ttf2__use_idf: True\n",
      "\tvect2__max_df: 0.9\n",
      "\tvect2__min_df: 2\n",
      "\tvect2__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3240 out of 3240 | elapsed:  2.3min finished\n",
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\neighbors\\base.py:217: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn. decomposition import LatentDirichletAllocation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "print(\"start classification with SVC...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect2', CountVectorizer(preprocessor = my_dummy,\n",
    "                              tokenizer =my_dummy, \n",
    "                              token_pattern=None)),\n",
    "                            \n",
    "    ('tf2',TfidfTransformer()),\n",
    "                            \n",
    "    ('clf2', KNeighborsClassifier()),    \n",
    "])\n",
    "\n",
    "parameters2 = {\n",
    "    'vect2__max_df': (0.9,0.95,1.0),\n",
    "    'vect2__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'vect2__min_df': (1,2,5),\n",
    "    'tf2__use_idf': (True, False),\n",
    "    'clf2__n_neighbors': (5,10), \n",
    "    'clf2__algorithm':('ball_tree', 'kd_tree'),\n",
    "    'clf2__leaf_size':(30,50,80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search2 = GridSearchCV(pipeline2, parameters2, cv=5,\n",
    "                           n_jobs = -1, verbose = 1)\n",
    "    print(\"performing gridsearch...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline2.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters2)\n",
    "    time_svc = time()\n",
    "    grid_search2.fit(X_train_preprocessed, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search2.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search2.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters2.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with MNB and extra LDA - Features...\n",
      "performing gridsearch...\n",
      "pipeline: ['union', 'clf3']\n",
      "parameters:\n",
      "{'union__pipe1__vect3__max_df': (0.9, 0.95, 1.0), 'union__pipe1__vect3__ngram_range': ((1, 1), (1, 2), (1, 3)), 'union__pipe1__vect3__min_df': (1, 2, 5), 'union__pipe1__tf3__use_idf': (True, False), 'union__pipe2__vect_lda__ngram_range': ((1, 1), (1, 2)), 'union__pipe2__lda1__n_components': (3, 6, 10)}\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed: 46.1min finished\n",
      "C:\\Users\\fmeyer\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2774.454s\n",
      "\n",
      "Best score: 0.652\n",
      "best parameters set:\n",
      "\tunion__pipe1__tf3__use_idf: True\n",
      "\tunion__pipe1__vect3__max_df: 0.9\n",
      "\tunion__pipe1__vect3__min_df: 5\n",
      "\tunion__pipe1__vect3__ngram_range: (1, 1)\n",
      "\tunion__pipe2__lda1__n_components: 6\n",
      "\tunion__pipe2__vect_lda__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn. decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "print(\"start classification with MNB and extra LDA - Features...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline3 = Pipeline([\n",
    "    ('union', FeatureUnion(n_jobs = -1,\n",
    "                          transformer_list = [\n",
    "                              ('pipe1', Pipeline([\n",
    "                                  ('vect3', CountVectorizer(preprocessor = my_dummy,\n",
    "                                                              tokenizer =my_dummy, \n",
    "                                                              token_pattern=None)),\n",
    "                                  ('tf3',TfidfTransformer()),\n",
    "                            \n",
    "    \n",
    "                              ])),\n",
    "                              ('pipe2', Pipeline([\n",
    "                                  ('vect_lda', CountVectorizer(preprocessor = my_dummy,\n",
    "                                                               tokenizer =my_dummy, token_pattern=None)),\n",
    "                                   ('lda1', LatentDirichletAllocation(max_iter = 100, learning_method = 'online', \n",
    "                                                                      learning_offset=50))\n",
    "                              ]))\n",
    "                            ])),\n",
    "                            \n",
    "    ('clf3', MultinomialNB()),    \n",
    "])\n",
    "\n",
    "parameters3 = {\n",
    "    'union__pipe1__vect3__max_df': (0.9,0.95,1.0),\n",
    "    'union__pipe1__vect3__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'union__pipe1__vect3__min_df': (1,2,5),\n",
    "    'union__pipe1__tf3__use_idf': (True, False),\n",
    "    'union__pipe2__vect_lda__ngram_range':((1,1),(1,2)),\n",
    "    'union__pipe2__lda1__n_components': (3,6,10),\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search3 = GridSearchCV(pipeline3, parameters3, cv=5,\n",
    "                           n_jobs = -1, verbose = 1)\n",
    "    print(\"performing gridsearch...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline3.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters3)\n",
    "    time_svc = time()\n",
    "    grid_search3.fit(X_train_preprocessed, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search3.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search3.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters3.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start classification with MNB and extra LDA - Features...\n",
      "performing gridsearch...\n",
      "pipeline: ['union1', 'clf4']\n",
      "parameters:\n",
      "{'union1__pipe3__vect4__max_df': (0.9, 0.95, 1.0), 'union1__pipe3__vect4__ngram_range': ((1, 1), (1, 2), (1, 3)), 'union1__pipe3__vect4__min_df': (1, 2, 5), 'union1__pipe3__tf4__use_idf': (True, False), 'union1__pipe4__vect_lda1__ngram_range': ((1, 1),), 'union1__pipe4__lda2__n_components': (3, 6), 'union1__pipe4__lda2__learning_offset': (5, 10, 20), 'clf4__kernel': ('linear',), 'clf4__C': (0.1, 1, 10, 100), 'clf4__max_iter': (100, 200)}\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 12.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-decd6aa01a98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mtime_svc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mgrid_search4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done in %0.3fs\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_svc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\classification_omt\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn. decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "print(\"start classification with MNB and extra LDA - Features...\")\n",
    "#define pipeline combining feature extractor with classifier\n",
    "pipeline4 = Pipeline([\n",
    "    ('union1', FeatureUnion(n_jobs = -1,\n",
    "                          transformer_list = [\n",
    "                              ('pipe3', Pipeline([\n",
    "                                  ('vect4', CountVectorizer(preprocessor = my_dummy,\n",
    "                                                              tokenizer =my_dummy, \n",
    "                                                              token_pattern=None)),\n",
    "                                  ('tf4',TfidfTransformer()),\n",
    "                            \n",
    "    \n",
    "                              ])),\n",
    "                              ('pipe4', Pipeline([\n",
    "                                  ('vect_lda1', CountVectorizer(preprocessor = my_dummy,\n",
    "                                                               tokenizer =my_dummy, token_pattern=None)),\n",
    "                                   ('lda2', LatentDirichletAllocation(max_iter = 100, learning_method = 'online'))\n",
    "                              ]))\n",
    "                            ])),\n",
    "                            \n",
    "    ('clf4', SVC()),    \n",
    "])\n",
    "\n",
    "parameters4 = {\n",
    "    'union1__pipe3__vect4__max_df': (0.9,0.95,1.0),\n",
    "    'union1__pipe3__vect4__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'union1__pipe3__vect4__min_df': (1,2,5),\n",
    "    'union1__pipe3__tf4__use_idf': (True, False),\n",
    "    'union1__pipe4__vect_lda1__ngram_range':((1,1),),\n",
    "    'union1__pipe4__lda2__n_components': (3,6),\n",
    "    'union1__pipe4__lda2__learning_offset': (5,10,20),\n",
    "    'clf4__kernel': ('linear',), \n",
    "    'clf4__C':(0.1,1,10,100),\n",
    "    'clf4__max_iter':(100,200),\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #find best params for this feature extraction methods\n",
    "    grid_search4 = GridSearchCV(pipeline4, parameters4, cv=5,\n",
    "                           n_jobs = -1, verbose = 1)\n",
    "    print(\"performing gridsearch...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline4.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters4)\n",
    "    time_svc = time()\n",
    "    grid_search4.fit(X_train_preprocessed, y_train)\n",
    "    print(\"done in %0.3fs\" % (time()-time_svc))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search4.best_score_)\n",
    "    print(\"best parameters set:\")\n",
    "    best_parameters = grid_search4.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters4.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
